{
    "模型量化": [
        {
            "slice_detail_type": "paragraph",
            "slice_hight": 12,
            "slice_type": 1,
            "slice_content": "模型量化GPTQAWQMIT、SJTU、清华联合提出的方法。该方法基于“权重不是同等重要”的观察，仅维护1%的显著权重可以大大减少量化的误差。AWQ不依赖于任何反向传播或重建，因此可以很好的保持LLM在不同领域和模式上的泛化能力，而不会过拟合到校准集。它也不依赖于任何数据布局重新排序，保持硬件效率。论文还实现了有效的张量核心内核，以加速AWQ的无重新排序在线反量化，实现速度比GPTQ快1.45核心思路：不是所有的权重都同等重要。1%的权重参数，可能会主导模型量化过程中的损失。保留这些参数的精度（FP16），会极大程度保护模型的性能。怎么选择1%的保留权重？论文发现根据activationmagnitude选择权重能显著提升模型效果。局限性：保留显著权重为FP16可以提升量化后模型的效果，但是这种方法对硬件不友好，会使模型效率降低。这一点和llm.int8()方法很像。通过激活感知缩放保护显著权重：",
            "pages": [
                1
            ],
            "location": [],
            "segment_id": 0,
            "id": "76349e1b-f5fa-4bc5-89fa-778286d605aa",
            "slice_md5": "ebb7a5727631e930f212a1a454226bb3",
            "superior_id": null,
            "subordinate_ids": [],
            "file_name": "模型量化"
        }
    ]
}